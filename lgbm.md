[toc]

## 决策树（Decision Tree）

###ID3 (Iterative Dichotomiser 3)

​	信息熵：$H(S)=\Sigma_{x \in X } -p(x) * log_2(p(x))$​， 其中 $X$​ 是数据集 $S$​ 中的不同类。在分类树中，$X$​ 就是不同的类别

​	在某一个节点选择分割特征的时候需要看条件熵，即选取的特征能使信息熵降得最低。

​	$H(X|Y) = H(X, Y) - H(X)$

​	信息增益 = 信息熵 - 条件熵.  $InfoGain(S, c) = H(S) - \Sigma_c \frac{|S_c|}{|S|} H(S_c)$​​ 

​	** ID3 的缺点

	1. 信息增益会趋向于选择类别比较多的属性，特征类别过多不一定有利于分类
	2. ID3 只适用于离散数据
	3. 不能处理缺失值，不能剪枝



### C4.5

​	由于信息增益的局限性，ID4.5 主要改进的就是选取特征的判断依据。

​	信息增益率：$GainRatio(S, f) = \frac{InfoGain(S, f)}{SplitInfo(S, f)}$​​​​，其中f为特征，$SplitInfo(S, f)$ 代表在选取特征$f$ 之后数据分布情况。

​	$SplitInfo(S, f) = -\Sigma_i \frac{|S_i|}{|S|} log_2 \frac{|S_i|}{|S|}$​

​	在选取特征之后，按照特征会分为不同的类别，当特征类别特别多的时候，$SplitInfo$​​ 值也会变大，一定程度上限制特征的选择



​	C4.5依然不能处理连续值的情况。



###CART （Classification and Regression Tree）

​	虽然C4.5 改进了特征的选取方法，但基于熵的计算涉及很多对数计算，在CART中使用了新的Gini指数作为选取标准。

​	$Gini(S) = 1-\Sigma_i p_i^2 = 1-\Sigma_i \frac{|D_i|^2}{|D|^2}$​​​  如果数据能被特征$f$ 分为两部分 $Gini(S, f) = \frac{|D1|}{|D|}Gini(D1)+ \frac{|D2|}{|D|}Gini(D2)$​

​	在特征分布接近平均时，基尼系数会比较大，因此使用基尼系数，在减小计算的同时，也会优先选取分割均匀的特征。



​	CART 在引入基尼系数的同时，还将之前按照特征值来分割为不同个数的单元，变成只分为两部分的二叉树。

​	

​	CART 在使用回归树的时候，由于规定只分裂为二叉树，一次可以定义连续值的处理。

 1. 选取一个特征，并排序

 2. 任意两个值之间作为分割点，可以有n-1种分法

 3. 计算每种情况的loss，并选取最小值，$Loss = \sum_i (f(X_i)-g_i)^2)$，其中$g_i$ 为叶结点上的均值，所以实际就是选取分裂以后两边方差和的最小的分裂值

 4. 选取最大增益的特征

    ​	

    使用回归树的时候，最终的结果是对应的Leaf Node的所有结果的均值，因此可以定义Loss Function

    

    

###缺失值处理

计算信息增益时，

- 计算信息熵, 忽略缺失值
- 计算信息增益, 乘以未缺失实例的比例

分裂的时候，缺失数据会继续走所有节点下面的分支上，每个分支的缺失数据有一个weight，没有缺失值的数据weight=1

对于有缺失特征的数据点，定义了几个weights

$\rho=\frac{|无缺失值数据|}{|所有数据|}$​​​

$p_k=\frac{｜无缺失数据中class(k)｜}{|所有无缺失数据|}$

$r_v=\frac{｜无缺失数据在特征a上取值a_v的数量｜}{｜无缺失数据在特征a上的数量｜}$​

$weight = \rho r_v$

N：叶结点样本权值总和，E：该叶结点与该数据label不同的权值总和

使用 $N/E$​ ​来表示，

* 其实就是所有叶结点上各个分类的概率取最大值



###剪枝

剪枝是树模型中有效的防止过拟合的方法，最简单的做法就是直接限制树的深度和叶结点的数量。另外，CART模型其实对于偏离散的数据更容易过拟合。因为在切分离散特征的时候，相当于一次性添加了很多非线性的效果。

后剪枝：

​	后剪枝的目的是在测试集上计算loss function，通过剪枝使loss function降低

​	主要是从叶结点开始从下往上，计算如果将跟结点下所有数据合并在一起loss function是否会降低。

​	后剪枝的Loss function添加了regularization项：$\alpha |T|$， 叶结点个数*$\alpha$ 。

​	$L_\alpha(T) = L(T) + \alpha|T|$ ， 剪枝以后的Loss：$L_\alpha(t) = L(t)+\alpha$  => $\alpha_{lim}=\frac{L(t)-L(T)}{|T|-1}$	





##GBDT

GBDT就是由一系列 Decision Tree 组成的加法模型，后续的树都是在减小前面结果的残差





##XGBoost





##LightGBM

首先LightGBM是一个boosting框架，基于数模型的学习算法（官网的描述）。。

